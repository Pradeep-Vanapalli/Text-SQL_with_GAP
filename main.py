#Importing the required Libraries
import json
import os
import _jsonnet
import psycopg2
import re
import os
import torch
import warnings
import pandas as pd
import plotly.express as px
warnings.filterwarnings("ignore")
from seq2struct.commands.infer import Inferer
from seq2struct.datasets.spider import SpiderItem
from seq2struct.utils import registry

#Configuration Loading
exp_config = json.loads(
    _jsonnet.evaluate_file(
    "experiments/spider-configs/gap-run.jsonnet"))
model_config_path = exp_config["model_config"]
model_config_args = exp_config.get("model_config_args")
infer_config = json.loads(
    _jsonnet.evaluate_file(
        model_config_path,
        tla_codes={'args': json.dumps(model_config_args)}))
infer_config["model"]["encoder_preproc"]["db_path"] = "data/sqlite_files/"

#Inference on the input
inferer = Inferer(infer_config)

#Pre-Trained Model checkpoints Loading
model_dir = exp_config["logdir"] + "/bs=12,lr=1.0e-04,bert_lr=1.0e-05,end_lr=0e0,att=1"
checkpoint_step = exp_config["eval_steps"][0]

model = inferer.load_model(model_dir, checkpoint_step)

from seq2struct.datasets.spider_lib.preprocess.get_tables import dump_db_json_schema
from seq2struct.datasets.spider import load_tables_from_schema_dict
from seq2struct.utils.api_utils import refine_schema_names

db_id = "Customers"
my_schema = dump_db_json_schema("data/sqlite_files/{db_id}/{db_id}.sqlite".format(db_id=db_id), db_id)
print(my_schema)

# If you want to change your schema name, then run this; Otherwise you can skip this.
#refine_schema_names(my_schema)

schema, eval_foreign_key_maps = load_tables_from_schema_dict(my_schema)
schema.keys()

dataset = registry.construct('dataset_infer', {
   "name": "spider", "schemas": schema, "eval_foreign_key_maps": eval_foreign_key_maps,
    "db_path": "data/sqlite_files/"
})

for _, schema in dataset.schemas.items():
    model.preproc.enc_preproc._preprocess_schema(schema)

spider_schema = dataset.schemas[db_id]

def infer(question):
    data_item = SpiderItem(
            text=None,  # intentionally None -- should be ignored when the tokenizer is set correctly
            code=None,
            schema=spider_schema,
            orig_schema=spider_schema.orig,
            orig={"question": question}
        )
    model.preproc.clear_items()
    enc_input = model.preproc.enc_preproc.preprocess_item(data_item, None)
    preproc_data = enc_input, None
    with torch.no_grad():
        output = inferer._infer_one(model, data_item, preproc_data, beam_size=1, use_heuristic=True)
    return output[0]["inferred_code"]

Questions = ["What is the average price of the product", "What is the phone numebr of the custoner", "What is the total quantity of product", "What is the address of the Kerrin Jambrozek", "What are all the product prices above 200"]
#Questions = ["Total revenue generated by the store", "Average value of each order placed on the store", "Percentage of visitors who complete a purchase on the store", "Predicted revenue a customer will generate during their lifetime on the store", " Revenue minus the cost of goods sold", "What are the top-selling products on our store?", "What are the prices of all products"]
#Questions = ["What are all products and price"]
#Questions = ["The total amount of money earned from sales over a specific period of time", " The average amount of money spent per order.", "The percentage of visitors to your store who make a purchase.", " The total amount of money a customer is expected to spend at your store over their lifetime."]
#Questions = ["What are the top-selling products on our store?", "What is the average order value on our store", " What is the sales trend for our store in 2022?", "What is the customer acquisition cost for each of our marketing channels?", "Which channels drive the most traffic to our store?"]

n = 2
for q in Questions:
    search_ngrams = [' '.join(q.split()[i:i + n]) for i in range(len(q.split()) - n + 1)]
    print(search_ngrams)
    code = infer(q)
    j = "'terminal'"
    for i in search_ngrams:
        i = f"'{i}'"
        #print(i)
        for d in i.split():
            d = d[:3]
            print(d)
            #print(type(d))
            if d.isdigit():
                print('ok')
                code = re.sub(i, d, code)
                j = i
                #print(f'in if: {j}')
            else:
                code = re.sub(j, i, code)
                j = i
                #print(f'in else: {j}')
    print(code)

    try:
        connection = psycopg2.connect(user="pgadmin",
                                          password="gqfxQz1AiAm",
                                          host="129.146.178.63",
                                          port="1555",
                                          database="airbyte")

        cursor = connection.cursor()
            # Print PostgreSQL Connection properties
            #print(connection.get_dsn_parameters(), "\n")

            # Print PostgreSQL version
            #cursor.execute("SELECT version();")

            #record = cursor.fetchone()
            #print("You are connected to - ", record, "\n")

        query = code
        cursor.execute(query)
        res=cursor.fetchall()
        print("Result is:" f'{res}')
        '''df = pd.DataFrame(res, columns=['product','price'])
        #df['product_id'] = df['product_id'].astype(str)
        print(df)
        fig = px.line(df.head(20), y='price')
        fig.show()'''
        print("Executed")

    except (Exception, psycopg2.Error) as error:
        print("Error while connecting to PostgreSQL", error)


    finally:
        # closing database connection.
        if (connection):
            cursor.close()
            connection.close()
            print("PostgreSQL connection is closed")

    continue

